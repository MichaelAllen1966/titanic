{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Receiver Operator Characteristic (ROC) curve and balancing of model classification\n",
    "\n",
    "In this model we apply ROC to a Random Forest model. As these components have been previously described, we'll keep comments to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings (to keep notebook tidy; do not usually do this)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data if not previously downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to calculate accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                                 np.sum(observed_positives))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                                  np.sum(observed_positives))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'\n",
    "# Convert to NumPy as required for k-fold splits\n",
    "X_np = X.values\n",
    "y_np = y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up k-fold training/test splits\n",
    "number_of_splits = 5\n",
    "skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "skf.get_n_splits(X_np, y_np)\n",
    "\n",
    "# Set up thresholds\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Create arrays for overall results (rows=threshold, columns=k fold replicate)\n",
    "results_accuracy = np.zeros((len(thresholds),number_of_splits))\n",
    "results_precision = np.zeros((len(thresholds),number_of_splits))\n",
    "results_recall = np.zeros((len(thresholds),number_of_splits))\n",
    "results_f1 = np.zeros((len(thresholds),number_of_splits))\n",
    "results_predicted_positive_rate = np.zeros((len(thresholds),number_of_splits))\n",
    "results_observed_positive_rate = np.zeros((len(thresholds),number_of_splits))\n",
    "results_true_positive_rate = np.zeros((len(thresholds),number_of_splits))\n",
    "results_false_positive_rate = np.zeros((len(thresholds),number_of_splits))\n",
    "results_auc = []\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "loop_index = 0\n",
    "for train_index, test_index in skf.split(X_np, y_np):\n",
    "    \n",
    "    # Create lists for k-fold results\n",
    "    threshold_accuracy = []\n",
    "    threshold_precision = []\n",
    "    threshold_recall = []\n",
    "    threshold_f1 = []\n",
    "    threshold_predicted_positive_rate = []\n",
    "    threshold_observed_positive_rate = []\n",
    "    threshold_true_positive_rate = []\n",
    "    threshold_false_positive_rate = []\n",
    "\n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X_np[train_index], X_np[test_index]\n",
    "    y_train, y_test = y_np[train_index], y_np[test_index]\n",
    "\n",
    "    # Set up and fit model (n_jobs=-1 uses all cores on a computer)\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    # Get probability of non-survive and survive\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "    # Take just the survival probabilities (column 1)\n",
    "    probability_survival = probabilities[:,1]\n",
    "    \n",
    "    # Loop through increments in probability of survival\n",
    "    for cutoff in thresholds: #  loop 0 --> 1 on steps of 0.1\n",
    "        # Get whether passengers survive using cutoff\n",
    "        predicted_survived = probability_survival >= cutoff\n",
    "        # Call accuracy measures function\n",
    "        accuracy = calculate_accuracy(y_test, predicted_survived)\n",
    "        # Add accuracy scores to lists\n",
    "        threshold_accuracy.append(accuracy['accuracy'])\n",
    "        threshold_precision.append(accuracy['precision'])\n",
    "        threshold_recall.append(accuracy['recall'])\n",
    "        threshold_f1.append(accuracy['f1'])\n",
    "        threshold_predicted_positive_rate.append(\n",
    "                accuracy['predicted_positive_rate'])\n",
    "        threshold_observed_positive_rate.append(\n",
    "                accuracy['observed_positive_rate'])\n",
    "        threshold_true_positive_rate.append(accuracy['true_positive_rate'])\n",
    "        threshold_false_positive_rate.append(accuracy['false_positive_rate'])\n",
    "    \n",
    "    # Add results to results arrays\n",
    "    results_accuracy[:,loop_index] = threshold_accuracy\n",
    "    results_precision[:, loop_index] = threshold_precision\n",
    "    results_recall[:, loop_index] = threshold_recall\n",
    "    results_f1[:, loop_index] = threshold_f1\n",
    "    results_predicted_positive_rate[:, loop_index] = \\\n",
    "        threshold_predicted_positive_rate\n",
    "    results_observed_positive_rate[:, loop_index] = \\\n",
    "        threshold_observed_positive_rate\n",
    "    results_true_positive_rate[:, loop_index] = threshold_true_positive_rate\n",
    "    results_false_positive_rate[:, loop_index] = threshold_false_positive_rate\n",
    "    \n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = auc(threshold_false_positive_rate, threshold_true_positive_rate)\n",
    "    results_auc.append(roc_auc)\n",
    "    \n",
    "    # Increment loop index\n",
    "    loop_index += 1\n",
    "    \n",
    "\n",
    "# Transfer summary results to dataframe\n",
    "results = pd.DataFrame(thresholds, columns=['thresholds'])\n",
    "results['accuracy'] = results_accuracy.mean(axis=1)\n",
    "results['precision'] = results_precision.mean(axis=1)\n",
    "results['recall'] = results_recall.mean(axis=1)\n",
    "results['f1'] = results_f1.mean(axis=1)\n",
    "results['predicted_positive_rate'] = \\\n",
    "    results_predicted_positive_rate.mean(axis=1)\n",
    "results['observed_positive_rate'] = \\\n",
    "    results_observed_positive_rate.mean(axis=1)\n",
    "results['true_positive_rate'] = results_true_positive_rate.mean(axis=1)\n",
    "results['false_positive_rate'] = results_false_positive_rate.mean(axis=1)\n",
    "results['roc_auc'] = np.mean(results_auc)\n",
    "\n",
    "mean_auc = np.mean(results_auc)\n",
    "mean_auc = np.round(mean_auc, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_splits):\n",
    "    plt.plot(results_false_positive_rate[:, i],\n",
    "             results_true_positive_rate[:, i],\n",
    "             color='black',\n",
    "             linestyle=':',\n",
    "             linewidth=1)\n",
    "plt.plot(results_false_positive_rate.mean(axis=1),\n",
    "         results_true_positive_rate.mean(axis=1),\n",
    "         color='red',\n",
    "         linestyle='-',\n",
    "         linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operator Characteristic (ROC) Curve')\n",
    "plt.grid(True)\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "text = \"Mean AUC = \" + str(mean_auc)\n",
    "plt.text(0.65, 0.08, text, bbox=props)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot effects of changing classification probability threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_x = results['thresholds']\n",
    "\n",
    "plt.plot(chart_x, results['accuracy'],\n",
    "         linestyle = '-',\n",
    "         label = 'Accuracy')\n",
    "\n",
    "plt.plot(chart_x, results['precision'],\n",
    "         linestyle = '--',\n",
    "         label = 'Precision')\n",
    "\n",
    "plt.plot(chart_x, results['recall'],\n",
    "         linestyle = '-.',\n",
    "         label = 'Recall')\n",
    "\n",
    "plt.plot(chart_x, results['f1'],\n",
    "         linestyle = ':',\n",
    "         label = 'F1')\n",
    "\n",
    "plt.plot(chart_x, results['predicted_positive_rate'],\n",
    "         linestyle = '-',\n",
    "         label = 'Predicted positive rate')\n",
    "\n",
    "plt.plot(chart_x, results['observed_positive_rate'],\n",
    "         linestyle = '--',\n",
    "         color='k',\n",
    "         label = 'Observed positive rate')\n",
    "\n",
    "\n",
    "plt.xlabel('Probability cutoff for classifcation')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(-0.02, 1.02)\n",
    "plt.legend(loc='lower left', ncol=2)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "* The Random Forest model, with default settings, has an ROC AUC of 0.85.\n",
    "* Unlike the logistic regression model, the default probability threshold of 0.5 produces a balanced model, where precision = recall, and where the predicted survival rate is the same as the observed survival rate.\n",
    "* If needed, the probability threshold of the Random Forest model could be adjust to change the model characteristics, such as for use in screening where recall (the detection of true positives) needs to be minimised at the accepted cost of lower precision (the detection of true negatives). In a screening situation false positives are generally accepted (as they will be eliminated later), but false negatives (where a condition is missed in screening) is much less acceptable.\n",
    "* You will see that many aspects of machine learning models are directly transferable between model types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
