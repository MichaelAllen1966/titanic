{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost as a replacement for Random Forest\n",
    "\n",
    "An alternative to SciKit Learn's Random Forest is XGBoost (`pip install xgboost` if needed). XGBoost is a development of Random Forests, and is often faster and may give added accuracy. We can use XGBoost as a drop in replacement for Random Forests, as shown here.\n",
    "\n",
    "This notebook is a repeat of the basic Random Forest model, but with Random Forest replaced by XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/miniconda3/envs/samuel/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "\n",
    "Run the following code if data for Titanic survival has not been previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & drop passenger ID\n",
    "data = pd.read_csv('data/processed_data.csv')\n",
    "\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "\n",
    "data.drop('PassengerId', inplace=True, axis=1)\n",
    "\n",
    "# Split data into two DataFrames\n",
    "X_df = data.drop('Survived',axis=1)\n",
    "y_df = data['Survived']\n",
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy(observed, predicted):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates a range of accuracy scores from observed and predicted classes.\n",
    "    \n",
    "    Takes two list or NumPy arrays (observed class values, and predicted class \n",
    "    values), and returns a dictionary of results.\n",
    "    \n",
    "     1) observed positive rate: proportion of observed cases that are +ve\n",
    "     2) Predicted positive rate: proportion of predicted cases that are +ve\n",
    "     3) observed negative rate: proportion of observed cases that are -ve\n",
    "     4) Predicted negative rate: proportion of predicted cases that are -ve  \n",
    "     5) accuracy: proportion of predicted results that are correct    \n",
    "     6) precision: proportion of predicted +ve that are correct\n",
    "     7) recall: proportion of true +ve correctly identified\n",
    "     8) f1: harmonic mean of precision and recall\n",
    "     9) sensitivity: Same as recall\n",
    "    10) specificity: Proportion of true -ve identified:        \n",
    "    11) positive likelihood: increased probability of true +ve if test +ve\n",
    "    12) negative likelihood: reduced probability of true +ve if test -ve\n",
    "    13) false positive rate: proportion of false +ves in true -ve patients\n",
    "    14) false negative rate: proportion of false -ves in true +ve patients\n",
    "    15) true positive rate: Same as recall\n",
    "    16) true negative rate: Same as specificity\n",
    "    17) positive predictive value: chance of true +ve if test +ve\n",
    "    18) negative predictive value: chance of true -ve if test -ve\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Converts list to NumPy arrays\n",
    "    if type(observed) == list:\n",
    "        observed = np.array(observed)\n",
    "    if type(predicted) == list:\n",
    "        predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate accuracy scores\n",
    "    observed_positives = observed == 1\n",
    "    observed_negatives = observed == 0\n",
    "    predicted_positives = predicted == 1\n",
    "    predicted_negatives = predicted == 0\n",
    "    \n",
    "    true_positives = (predicted_positives == 1) & (observed_positives == 1)\n",
    "    \n",
    "    false_positives = (predicted_positives == 1) & (observed_positives == 0)\n",
    "    \n",
    "    true_negatives = (predicted_negatives == 1) & (observed_negatives == 1)\n",
    "    \n",
    "    false_negatives = (predicted_negatives == 1) & (observed_negatives == 0)\n",
    "    \n",
    "    accuracy = np.mean(predicted == observed)\n",
    "    \n",
    "    precision = (np.sum(true_positives) /\n",
    "                 (np.sum(true_positives) + np.sum(false_positives)))\n",
    "        \n",
    "    recall = np.sum(true_positives) / np.sum(observed_positives)\n",
    "    \n",
    "    sensitivity = recall\n",
    "    \n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    \n",
    "    specificity = np.sum(true_negatives) / np.sum(observed_negatives)\n",
    "    \n",
    "    positive_likelihood = sensitivity / (1 - specificity)\n",
    "    \n",
    "    negative_likelihood = (1 - sensitivity) / specificity\n",
    "    \n",
    "    false_positive_rate = 1 - specificity\n",
    "    \n",
    "    false_negative_rate = 1 - sensitivity\n",
    "    \n",
    "    true_positive_rate = sensitivity\n",
    "    \n",
    "    true_negative_rate = specificity\n",
    "    \n",
    "    positive_predictive_value = (np.sum(true_positives) / \n",
    "                            (np.sum(true_positives) + np.sum(false_positives)))\n",
    "    \n",
    "    negative_predictive_value = (np.sum(true_negatives) / \n",
    "                            (np.sum(true_negatives) + np.sum(false_negatives)))\n",
    "    \n",
    "    # Create dictionary for results, and add results\n",
    "    results = dict()\n",
    "    \n",
    "    results['observed_positive_rate'] = np.mean(observed_positives)\n",
    "    results['observed_negative_rate'] = np.mean(observed_negatives)\n",
    "    results['predicted_positive_rate'] = np.mean(predicted_positives)\n",
    "    results['predicted_negative_rate'] = np.mean(predicted_negatives)\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1'] = f1\n",
    "    results['sensitivity'] = sensitivity\n",
    "    results['specificity'] = specificity\n",
    "    results['positive_likelihood'] = positive_likelihood\n",
    "    results['negative_likelihood'] = negative_likelihood\n",
    "    results['false_positive_rate'] = false_positive_rate\n",
    "    results['false_negative_rate'] = false_negative_rate\n",
    "    results['true_positive_rate'] = true_positive_rate\n",
    "    results['true_negative_rate'] = true_negative_rate\n",
    "    results['positive_predictive_value'] = positive_predictive_value\n",
    "    results['negative_predictive_value'] = negative_predictive_value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model with k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up lists to hold results for each k-fold run\n",
    "replicate_accuracy = []\n",
    "replicate_precision = []\n",
    "replicate_recall = []\n",
    "replicate_f1 = []\n",
    "replicate_predicted_positive_rate = []\n",
    "replicate_observed_positive_rate = []\n",
    "\n",
    "# Set up DataFrame for feature importances\n",
    "importances = pd.DataFrame(index = list(X_df))\n",
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "# Set up splits\n",
    "number_of_splits = 10\n",
    "skf = StratifiedKFold(n_splits = number_of_splits)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# Loop through the k-fold splits\n",
    "k_fold_count = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    # Get X and Y train/test\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Set up and fit model (n_jobs=-1 uses all cores on a computer)\n",
    "    model = XGBClassifier(random_state=42, verbosity=0)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict test set labels and get accuracy scores\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    accuracy_scores = calculate_accuracy(y_test, y_pred_test)\n",
    "    replicate_accuracy.append(accuracy_scores['accuracy'])\n",
    "    replicate_precision.append(accuracy_scores['precision'])\n",
    "    replicate_recall.append(accuracy_scores['recall'])\n",
    "    replicate_f1.append(accuracy_scores['f1'])\n",
    "    replicate_predicted_positive_rate.append(\n",
    "        accuracy_scores['predicted_positive_rate'])\n",
    "    replicate_observed_positive_rate.append(\n",
    "        accuracy_scores['observed_positive_rate'])\n",
    "    \n",
    "    # Record feature importances\n",
    "    col_title = 'split_' + str(k_fold_count)\n",
    "    importances[col_title] = model.feature_importances_\n",
    "    k_fold_count +=1\n",
    "    \n",
    "# Transfer results to list and add to data frame\n",
    "results = pd.Series()\n",
    "results['accuracy'] = np.mean(replicate_accuracy)\n",
    "results['precision'] = np.mean(replicate_precision)\n",
    "results['recall'] = np.mean(replicate_recall)\n",
    "results['f1'] = np.mean(replicate_f1)\n",
    "results['predicted positive rate'] = np.mean(replicate_predicted_positive_rate)\n",
    "results['observed positive rate'] = np.mean(replicate_observed_positive_rate)\n",
    "\n",
    "# Get average of feature importances, and sort\n",
    "importance_mean = importances.mean(axis=1)\n",
    "importance_mean.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy                   0.820449\n",
       "precision                  0.776687\n",
       "recall                     0.745630\n",
       "f1                         0.758968\n",
       "predicted positive rate    0.368102\n",
       "observed positive rate     0.383833\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male                   0.409115\n",
       "Pclass                 0.151310\n",
       "CabinLetter_E          0.077558\n",
       "CabinNumber            0.047122\n",
       "SibSp                  0.042579\n",
       "Embarked_S             0.036095\n",
       "Embarked_C             0.029504\n",
       "CabinLetter_A          0.028807\n",
       "Fare                   0.027939\n",
       "Age                    0.027254\n",
       "CabinLetter_C          0.024650\n",
       "Parch                  0.021589\n",
       "CabinLetterImputed     0.020404\n",
       "AgeImputed             0.018040\n",
       "CabinLetter_B          0.013238\n",
       "CabinLetter_D          0.013003\n",
       "Embarked_Q             0.011792\n",
       "Embarked_missing       0.000000\n",
       "CabinNumberImputed     0.000000\n",
       "EmbarkedImputed        0.000000\n",
       "CabinLetter_F          0.000000\n",
       "CabinLetter_G          0.000000\n",
       "CabinLetter_T          0.000000\n",
       "CabinLetter_missing    0.000000\n",
       "dtype: float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('samuel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b368e36a85415766688ec72e3e874a4b525584eabf4bf7122952a4e0fd64fcde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
